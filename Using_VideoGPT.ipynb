{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmaruEscalante/VideoGPT/blob/master/Using_VideoGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXfZXsNhy08r"
      },
      "source": [
        "# Using VideoGPT\n",
        "This is a notebook demonstrating how to use VideoGPT and any pretrained models, Make sure that it is a GPU instance: **Change Runtime Type -> GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ponLMda7zBmF"
      },
      "source": [
        "## Installation\n",
        "First, we install the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AqQFVVvT-iE",
        "outputId": "683639c3-441d-45d2-e2cb-79b2159d7d31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/VideoGPT\n"
          ]
        }
      ],
      "source": [
        "%cd VideoGPT/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnfW3fATNRmN",
        "outputId": "eb920e2e-451f-4f86-beec-84f8be903fd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'VideoGPT'...\n",
            "remote: Enumerating objects: 398, done.\u001b[K\n",
            "remote: Counting objects: 100% (150/150), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 398 (delta 79), reused 97 (delta 62), pack-reused 248\u001b[K\n",
            "Receiving objects: 100% (398/398), 4.05 MiB | 24.83 MiB/s, done.\n",
            "Resolving deltas: 100% (219/219), done.\n",
            "/content/VideoGPT/VideoGPT\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/amaruescalante/VideoGPT.git\n",
        "%cd VideoGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD86SgJ1yvwZ"
      },
      "outputs": [],
      "source": [
        "! pip install git+https://github.com/amaruescalante/VideoGPT.git\n",
        "! pip install scikit-video ava\n",
        "! pip install --upgrade --no-cache-dir gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2x57LehHGRV"
      },
      "outputs": [],
      "source": [
        "!sh scripts/preprocess/ucf101/create_ucf_dataset.sh datasets/ucf101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxK8FfUoXOQJ"
      },
      "outputs": [],
      "source": [
        "!sh scripts/preprocess/msrvtt/create_msrvtt_dataset.sh datasets/msrvtt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdbqllwlDKec"
      },
      "outputs": [],
      "source": [
        "# Train VQ-VAE\n",
        "! python scripts/train_vqvae.py --data_path datasets/msrvtt --accelerator gpu --batch_size 16 --gpus 1 --auto_select_gpus true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArmpumI-zeHK"
      },
      "outputs": [],
      "source": [
        "! python scripts/train_videogpt.py --data_path datasets/msrvtt --accelerator gpu --batch_size 16 --gpus 1 --auto_select_gpus true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G1lfDiwycLl"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torchvision.io import read_video, read_video_timestamps\n",
        "\n",
        "from videogpt import download, load_vqvae, load_videogpt\n",
        "from videogpt.data import preprocess\n",
        "\n",
        "VIDEOS = {\n",
        "    'breakdancing': '1OZBnG235-J9LgB_qHv-waHZ4tjofiDgj',\n",
        "    'bear': '16nIaqq2vbPh-WMo_7hs9feVSe0jWVXLF',\n",
        "    'jaywalking': '1UxKCVrbyXhvMz_H7dI4w5hjPpRGCAApy',\n",
        "    'cartoon': '1ONcTMSEuGuLYIDbX-KeFqd390vbTIH9d'\n",
        "}\n",
        "\n",
        "ROOT = 'pretrained_models'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_RyF9AU2tzK"
      },
      "source": [
        "## Downloading a Pretrained VQ-VAE\n",
        "There are four pretrained models available: `bair_stride4x2x2`, `ucf101_stride4x4x4`, `kinetics_stride4x4x4`, and `kinetics_stride2x4x4`. BAIR was trained on 64 x 64 video, and the rest on 128 x 128. The `stride` component represents the THW downsampling the VQ-VAE performs on the video tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t5fEML30L3f",
        "outputId": "74c3bda3-bd02-470b-9b44-58c42cc78972"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/migration/migration.py:195: PossibleUserWarning: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
            "  rank_zero_warn(\n",
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.1.1 to v1.9.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../root/.cache/videogpt/ucf101_stride4x4x4`\n"
          ]
        }
      ],
      "source": [
        "%reload_ext autoreload\n",
        "from videogpt.vqvae import VQVAE\n",
        "device = torch.device('cuda')\n",
        "# vqvae = load_vqvae('kinetics_stride2x4x4', device=device, root=ROOT).to(device)\n",
        "# vqvae = load_vqvae('ucf101_stride4x4x4', device=device, root=ROOT).to(device)\n",
        "\n",
        "# Download VQ-VAE\n",
        "filepath = download(\"1FNWJtWDTX5CcVSSlINK1ZFFHuBgjBZfB\", \"ucf101_stride4x4x4\")\n",
        "vqvae = VQVAE.load_from_checkpoint(filepath).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRWcHSIwYkO"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QMgNCPo3jQg"
      },
      "source": [
        "## Video Loading and Preprocessing\n",
        "The code below downloads, loads, and preprocesses a given `mp4` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "X_FYfsIf2kwU",
        "outputId": "5313b8e1-8fe1-47b9-bad3-e7e266f92406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Access denied with the following error:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " \tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1UxKCVrbyXhvMz_H7dI4w5hjPpRGCAApy \n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/io/video.py:408: RuntimeWarning: Failed to open container for /root/.cache/videogpt/jaywalking.mp4; Caught error: [Errno 2] No such file or directory: '/root/.cache/videogpt/jaywalking.mp4'\n",
            "  warnings.warn(msg, RuntimeWarning)\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4c3ff7ce80ac>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvideo_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVIDEOS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideo_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{video_name}.mp4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_video_timestamps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpts_unit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpts_unit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_pts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_pts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msequence_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "%reload_ext autoreload\n",
        "video_name = 'jaywalking'\n",
        "# `resolution` must be divisible by the encoder image stride\n",
        "# `sequence_length` must be divisible by the encoder temporal stride\n",
        "resolution, sequence_length = vqvae.args.resolution, 16\n",
        "\n",
        "video_filename = download(VIDEOS[video_name], f'{video_name}.mp4')\n",
        "pts = read_video_timestamps(video_filename, pts_unit='sec')[0]\n",
        "video = read_video(video_filename, pts_unit='sec', start_pts=pts[0], end_pts=pts[sequence_length - 1])[0]\n",
        "video = preprocess(video, resolution, sequence_length).unsqueeze(0).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA3R-ZOi3uri"
      },
      "source": [
        "## VQ-VAE Encoding and Decoding\n",
        "Now, we can encode the video through the `encode` function. The `encode` function also has an optional input `including_embeddings` (default `False`) which will also return the embedding versions of the encodings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "ywTjc5wi2odm",
        "outputId": "1b87d51f-3d05-4087-d7ea-150bab20779f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-efbe44c75e24>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvqvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mvideo_recon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvqvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvideo_recon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'video' is not defined"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    encodings = vqvae.encode(video)\n",
        "    video_recon = vqvae.decode(encodings)\n",
        "    video_recon = torch.clamp(video_recon, -0.5, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcyzbBVX4J-d"
      },
      "source": [
        "## Visualizing Reconstructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "Y2t-dwme2qN1",
        "outputId": "2080ed9e-51ab-4cdc-da10-00db0cc7bd74"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8a48c71191c7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvideos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_recon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvideos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# CTHW -> THWC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvideos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'video' is not defined"
          ]
        }
      ],
      "source": [
        "videos = torch.cat((video, video_recon), dim=-1)\n",
        "videos = videos[0].permute(1, 2, 3, 0) # CTHW -> THWC\n",
        "videos = ((videos + 0.5) * 255).cpu().numpy().astype('uint8')\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.title('real (left), reconstruction (right)')\n",
        "plt.axis('off')\n",
        "im = plt.imshow(videos[0, :, :, :])\n",
        "plt.close()\n",
        "\n",
        "def init():\n",
        "    im.set_data(videos[0, :, :, :])\n",
        "\n",
        "def animate(i):\n",
        "    im.set_data(videos[i, :, :, :])\n",
        "    return im\n",
        "\n",
        "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=videos.shape[0], interval=200) # 200ms = 5 fps\n",
        "HTML(anim.to_html5_video())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz3EFC-ZXL7z"
      },
      "source": [
        "# Using Pretrained VideoGPT Models\n",
        "\n",
        "The current available model to download is `ucf101`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95FevjM0XG_y"
      },
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "from videogpt.gpt import VideoGPT\n",
        "from videogpt import download, load_vqvae, load_videogpt\n",
        "device = torch.device('cuda')\n",
        "filepath = download(\"1c4CYL1joN5KDC5VYJIilFYWcDOmjWtgE\", \"ucf101_uncond_gpt\")\n",
        "gpt = VideoGPT.load_from_checkpoint(filepath).to(device)\n",
        "gpt.eval()\n",
        "# gpt.eval()\n",
        "# gpt = load_videogpt('ucf101_uncond_gpt', device=device).to(device)\n",
        "# gpt = load_videogpt('bair_gpt', device=device).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMlpXebRY3P5"
      },
      "source": [
        "`VideoGPT.sample` method returns generated samples of shape BCTHW in the range [0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSqFaYfcxcT-"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install llvm-9-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaL-7uz5yKmy"
      },
      "outputs": [],
      "source": [
        "%cd VideoGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3o2aPtJQXnjX",
        "outputId": "a40b36d7-b32f-4a7d-c1e0-3d3990be42eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4096/4096 [02:13<00:00, 30.70it/s]\n"
          ]
        }
      ],
      "source": [
        "samples = gpt.sample(16) # unconditional model does not require batch input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZxFIFYzY_Kj"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "b, c, t, h, w = samples.shape\n",
        "samples = samples.permute(0, 2, 3, 4, 1)\n",
        "samples = (samples.cpu().numpy() * 255).astype('uint8')\n",
        "\n",
        "video = np.zeros((t, (1 + h) * 4 + 1, (1 + w) * 4 + 1, c), dtype='uint8')\n",
        "for i in range(b):\n",
        "  r, c = i // 4, i % 4\n",
        "  start_r, start_c = (1 + h) * r, (1 + w) * c\n",
        "  video[:, start_r:start_r + h, start_c:start_c + w] = samples[i]\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.title('ucf101 unconditional samples')\n",
        "plt.axis('off')\n",
        "im = plt.imshow(video[0, :, :, :])\n",
        "plt.close()\n",
        "\n",
        "def init():\n",
        "    im.set_data(video[0, :, :, :])\n",
        "\n",
        "def animate(i):\n",
        "    im.set_data(video[i, :, :, :])\n",
        "    return im\n",
        "\n",
        "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0], interval=200) # 200ms = 5 fps\n",
        "HTML(anim.to_html5_video())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJZWg67x3pDB"
      },
      "source": [
        "# Computing FVD on UCF101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqBkGfeTCtt6"
      },
      "outputs": [],
      "source": [
        "!git clone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNYMtXBx7d3E",
        "outputId": "3ad7753d-4137-4835-82c2-2327792bf9c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'VideoGPT'...\n",
            "remote: Enumerating objects: 398, done.\u001b[K\n",
            "remote: Counting objects: 100% (133/133), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 398 (delta 62), reused 81 (delta 46), pack-reused 265\u001b[K\n",
            "Receiving objects: 100% (398/398), 4.05 MiB | 20.32 MiB/s, done.\n",
            "Resolving deltas: 100% (219/219), done.\n",
            "/content/VideoGPT\n"
          ]
        }
      ],
      "source": [
        "# !pip install git+https://github.com/amaruescalante/VideoGPT.git@ff13f8b43b316086fa04d3adf468d187ceecac76\n",
        "# !pip install git+https://github.com/amaruescalante/VideoGPT.git\n",
        "!git clone https://github.com/amaruescalante/VideoGPT.git\n",
        "%cd VideoGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3NnCJsrC5NK"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ttWAbuX5eG_"
      },
      "outputs": [],
      "source": [
        "from videogpt.download import load_i3d_pretrained\n",
        "from videogpt.fvd.fvd import get_fvd_logits, frechet_distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMMcfyVK_ipf"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "i3d = load_i3d_pretrained(device=torch.device(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGlFB-P134CQ"
      },
      "outputs": [],
      "source": [
        "%reload_ext autoreload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajtJjATGJjK7",
        "outputId": "e63b9c01-feab-4b49-db10-28d6854dc2f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hparams <class 'argparse.Namespace'>\n"
          ]
        }
      ],
      "source": [
        "hparams = gpt.hparams['args']\n",
        "print(\"hparams\", type(hparams))\n",
        "hparams.batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ejn_6ISOocJ",
        "outputId": "a5534189-6181-43d3-ee42-2f89a4597dab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [01:14<?, ?it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "19095"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx6Mwn9Q3QoU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from videogpt.download import load_i3d_pretrained\n",
        "# from tqdm import tqdm # this is for script version\n",
        "from tqdm.notebook import tqdm  # Use this version of tqdm for Jupyter notebooks\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "from videogpt.fvd.fvd import get_fvd_logits, frechet_distance\n",
        "from videogpt import VideoData, VideoGPT, load_videogpt\n",
        "\n",
        "MAX_BATCH = 4\n",
        "\n",
        "def main(ckpt='bair_gpt', n_trials=1, port=23452):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    #################### Load VideoGPT ########################################\n",
        "    # if not os.path.exists(ckpt):\n",
        "        # gpt = load_videogpt(ckpt, device=device)\n",
        "    # else:\n",
        "        # gpt = VideoGPT.load_from_checkpoint(ckpt).to(device)\n",
        "    gpt.eval()\n",
        "    hparams = gpt.hparams['args']\n",
        "    # print(\"hparams\", hparams)\n",
        "    batch_size = 4\n",
        "    hparams.batch_size = batch_size\n",
        "    loader = VideoData(hparams).test_dataloader()\n",
        "\n",
        "    #################### Load I3D ########################################\n",
        "    i3d = load_i3d_pretrained(device)\n",
        "\n",
        "    #################### Compute FVD ###############################\n",
        "    fvds = []\n",
        "    fvds_star = []\n",
        "    pbar = tqdm(total=n_trials)\n",
        "    for _ in range(n_trials):\n",
        "        fvd, fvd_star = eval_fvd(i3d, gpt, loader, device)\n",
        "        fvds.append(fvd)\n",
        "        fvds_star.append(fvd_star)\n",
        "\n",
        "        pbar.update(1)\n",
        "        fvd_mean = np.mean(fvds)\n",
        "        fvd_std = np.std(fvds)\n",
        "\n",
        "        fvd_star_mean = np.mean(fvds_star)\n",
        "        fvd_star_std = np.std(fvds_star)\n",
        "\n",
        "        pbar.set_description(f\"FVD {fvd_mean:.2f} +/- {fvd_std:.2f}, FVD* {fvd_star_mean:.2f} +/- {fvd_star_std:.2f}\")\n",
        "    pbar.close()\n",
        "    print(f\"Final FVD {fvd_mean:.2f} +/- {fvd_std:.2f}, FVD* {fvd_star_mean:.2f} +/- {fvd_star_std:.2f}\")\n",
        "\n",
        "def all_gather(tensor):\n",
        "    rank, size = dist.get_rank(), dist.get_world_size()\n",
        "    tensor_list = [torch.zeros_like(tensor) for _ in range(size)]\n",
        "    dist.all_gather(tensor_list, tensor)\n",
        "    return torch.cat(tensor_list)\n",
        "\n",
        "\n",
        "def eval_fvd(i3d, videogpt, loader, device):\n",
        "    # rank, size = dist.get_rank(), dist.get_world_size()  # Removed distributed parts\n",
        "    # is_root = rank == 0  # Not needed in sequential execution\n",
        "\n",
        "    batch = next(iter(loader))\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "    fake_embeddings = []\n",
        "    for i in range(0, batch['video'].shape[0], MAX_BATCH):\n",
        "        fake = videogpt.sample(MAX_BATCH, {k: v[i:i+MAX_BATCH] for k, v in batch.items()})\n",
        "        fake = torch.repeat_interleave(fake, 4, dim=2) # TODO: check correctness\n",
        "        fake = fake.permute(0, 2, 3, 4, 1).cpu().numpy() # BCTHW -> BTHWC\n",
        "        fake = (fake * 255).astype('uint8')\n",
        "        fake_embeddings.append(get_fvd_logits(fake, i3d=i3d, device=device))\n",
        "    fake_embeddings = torch.cat(fake_embeddings)\n",
        "\n",
        "    real = batch['video'].to(device)\n",
        "    real_recon_embeddings = []\n",
        "    for i in range(0, batch['video'].shape[0], MAX_BATCH):\n",
        "        real_recon = (videogpt.get_reconstruction(batch['video'][i:i+MAX_BATCH]) + 0.5).clamp(0, 1)\n",
        "        real_recon = torch.repeat_interleave(real_recon, 4, dim=2)\n",
        "        real_recon = real_recon.permute(0, 2, 3, 4, 1).cpu().numpy()\n",
        "        real_recon = (real_recon * 255).astype('uint8')\n",
        "        real_recon_embeddings.append(get_fvd_logits(real_recon, i3d=i3d, device=device))\n",
        "    real_recon_embeddings = torch.cat(real_recon_embeddings)\n",
        "\n",
        "    real = real + 0.5\n",
        "    real = real.permute(0, 2, 3, 4, 1).cpu().numpy() # BCTHW -> BTHWC\n",
        "    real = (real * 255).astype('uint8')\n",
        "    real_embeddings = get_fvd_logits(real, i3d=i3d, device=device)\n",
        "\n",
        "    # fake_embeddings = all_gather(fake_embeddings)  # Not needed in sequential execution\n",
        "    # real_recon_embeddings = all_gather(real_recon_embeddings)  # Not needed in sequential execution\n",
        "    # real_embeddings = all_gather(real_embeddings)  # Not needed in sequential execution\n",
        "\n",
        "    # Ensure that fake_embeddings and real_embeddings have the same number of items\n",
        "    assert fake_embeddings.shape[0] == real_recon_embeddings.shape[0] == real_embeddings.shape[0]\n",
        "\n",
        "    fvd = frechet_distance(fake_embeddings.clone(), real_embeddings)\n",
        "    fvd_star = frechet_distance(fake_embeddings.clone(), real_recon_embeddings)\n",
        "    return fvd.item(), fvd_star.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e1f3a0ff82a2462ba017929a8afe5c42",
            "95c95f5473d94329a786a8c70001413f",
            "1e875983529c48d1bed59f7145cc052d",
            "157b8086b288464ba7632d84d236dfd1",
            "32e913100ae6422f9be73fd269145773",
            "756fe1b4073c42e1b1defc7063264556",
            "2cfc446d25854cbd91a96fdb4983760c",
            "90110cbfcc2946ba88ee390d117c67cd",
            "bd35d508213445ed9ffb2b8e30a7db18",
            "c1694a466fc844109f5f9f749c21a2b6",
            "2750de0c120f4c35a50511be890e26f8"
          ]
        },
        "id": "09BfehFTE9ak",
        "outputId": "d338bddb-152d-4191-8f29-a31a2ff96ac2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1f3a0ff82a2462ba017929a8afe5c42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "main(ckpt='ufc', n_trials=10, port=12345)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "157b8086b288464ba7632d84d236dfd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1694a466fc844109f5f9f749c21a2b6",
            "placeholder": "​",
            "style": "IPY_MODEL_2750de0c120f4c35a50511be890e26f8",
            "value": " 0/10 [00:00&lt;?, ?it/s]"
          }
        },
        "1e875983529c48d1bed59f7145cc052d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90110cbfcc2946ba88ee390d117c67cd",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd35d508213445ed9ffb2b8e30a7db18",
            "value": 0
          }
        },
        "2750de0c120f4c35a50511be890e26f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cfc446d25854cbd91a96fdb4983760c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32e913100ae6422f9be73fd269145773": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "756fe1b4073c42e1b1defc7063264556": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90110cbfcc2946ba88ee390d117c67cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95c95f5473d94329a786a8c70001413f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_756fe1b4073c42e1b1defc7063264556",
            "placeholder": "​",
            "style": "IPY_MODEL_2cfc446d25854cbd91a96fdb4983760c",
            "value": "  0%"
          }
        },
        "bd35d508213445ed9ffb2b8e30a7db18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1694a466fc844109f5f9f749c21a2b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1f3a0ff82a2462ba017929a8afe5c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95c95f5473d94329a786a8c70001413f",
              "IPY_MODEL_1e875983529c48d1bed59f7145cc052d",
              "IPY_MODEL_157b8086b288464ba7632d84d236dfd1"
            ],
            "layout": "IPY_MODEL_32e913100ae6422f9be73fd269145773"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}